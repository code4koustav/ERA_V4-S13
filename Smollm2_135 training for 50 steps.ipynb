{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers==4.40.1\" huggingface_hub sentencepiece tokenizers"
      ],
      "metadata": {
        "id": "MROfGW9SQbd6",
        "outputId": "5363c68e-98cc-47fd-e860-391b55108f51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.1 in /usr/local/lib/python3.12/dist-packages (4.40.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.40.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.40.1) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Resume Training SmolLM2-135M from a Checkpoint\n",
        "Fixes:\n",
        "- Dataset chunk size issues\n",
        "- PyTorch 2.6+ safe loading of checkpoints\n",
        "- Handles token sequences safely for max_position_embeddings\n",
        "- Mixed precision support\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from dataclasses import dataclass, fields\n",
        "import json, os\n",
        "from tqdm import tqdm\n",
        "from torch.serialization import add_safe_globals\n",
        "\n",
        "# =============================\n",
        "# Device Configuration\n",
        "# =============================\n",
        "def get_device_config():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\"), True, torch.bfloat16, f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\"), True, torch.float16, \"Apple Silicon (MPS)\"\n",
        "    else:\n",
        "        return torch.device(\"cpu\"), False, torch.float32, \"CPU\"\n",
        "\n",
        "# =============================\n",
        "# Load config from JSON\n",
        "# =============================\n",
        "@dataclass\n",
        "class SmolLM2Config:\n",
        "    vocab_size: int = 49152\n",
        "    hidden_size: int = 576\n",
        "    intermediate_size: int = 1536\n",
        "    num_hidden_layers: int = 30\n",
        "    num_attention_heads: int = 9\n",
        "    num_key_value_heads: int = 3\n",
        "    max_position_embeddings: int = 8192\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 100000.0\n",
        "    attention_dropout: float = 0.0\n",
        "    hidden_dropout: float = 0.0\n",
        "    initializer_range: float = 0.041666666666666664\n",
        "    tie_word_embeddings: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.head_dim = self.hidden_size // self.num_attention_heads\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, path):\n",
        "        with open(path, \"r\") as f:\n",
        "            cfg = json.load(f)\n",
        "        allowed_keys = {f.name for f in fields(cls)}\n",
        "        filtered_cfg = {k: v for k, v in cfg.items() if k in allowed_keys}\n",
        "        return cls(**filtered_cfg)\n",
        "\n",
        "# =============================\n",
        "# Dataset\n",
        "# =============================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=32, batch_size=4):\n",
        "        text = open(file_path, \"r\", encoding=\"utf-8\").read()\n",
        "        ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        tp = block_size * batch_size\n",
        "\n",
        "        # Trim to multiple of (tp+1)\n",
        "        total_tokens = len(ids) - ((len(ids) - 1) % (tp + 1))\n",
        "        ids = ids[:total_tokens]\n",
        "\n",
        "        self.data = torch.tensor(ids, dtype=torch.long)\n",
        "        self.num_batches = len(self.data) // (tp + 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tp = self.block_size * self.batch_size\n",
        "        start = idx * (tp + 1)\n",
        "        chunk = self.data[start:start + tp + 1]\n",
        "        if len(chunk) != tp + 1:\n",
        "            # pad last chunk (safety)\n",
        "            pad_len = tp + 1 - len(chunk)\n",
        "            chunk = torch.cat([chunk, torch.zeros(pad_len, dtype=torch.long)])\n",
        "        x = chunk[:-1].view(self.batch_size, self.block_size)\n",
        "        y = chunk[1:].view(self.batch_size, self.block_size)\n",
        "        return x, y\n",
        "\n",
        "# =============================\n",
        "# SmolLM2 Model Classes\n",
        "# =============================\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        var = x.pow(2).mean(-1, keepdim=True)\n",
        "        x = x * torch.rsqrt(var + self.eps)\n",
        "        return self.weight * x\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings=8192, base=100000.0):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.base = base\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "    def forward(self, seq_len, device):\n",
        "        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb.cos()[None, :, :], emb.sin()[None, :, :]\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2:]\n",
        "    return torch.cat([-x2, x1], dim=-1)\n",
        "\n",
        "def apply_rotary(q, k, cos, sin):\n",
        "    return (q * cos + rotate_half(q) * sin, k * cos + rotate_half(k) * sin)\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, cfg: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.q_proj = nn.Linear(cfg.hidden_size, cfg.num_attention_heads * cfg.head_dim, bias=False)\n",
        "        self.k_proj = nn.Linear(cfg.hidden_size, cfg.num_key_value_heads * cfg.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(cfg.hidden_size, cfg.num_key_value_heads * cfg.head_dim, bias=False)\n",
        "        self.o_proj = nn.Linear(cfg.num_attention_heads * cfg.head_dim, cfg.hidden_size, bias=False)\n",
        "        self.rotary = RotaryEmbedding(cfg.head_dim, max_position_embeddings=cfg.max_position_embeddings, base=cfg.rope_theta)\n",
        "        self.num_key_value_groups = cfg.num_attention_heads // cfg.num_key_value_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.shape\n",
        "        q = self.q_proj(x).view(B, T, self.cfg.num_attention_heads, self.cfg.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.cfg.num_key_value_heads, self.cfg.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.cfg.num_key_value_heads, self.cfg.head_dim).transpose(1, 2)\n",
        "        cos, sin = self.rotary(T, x.device)\n",
        "        q, k = apply_rotary(q, k, cos, sin)\n",
        "        k = k.repeat_interleave(self.num_key_value_groups, dim=1)\n",
        "        v = v.repeat_interleave(self.num_key_value_groups, dim=1)\n",
        "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.cfg.attention_dropout if self.training else 0.0, is_causal=True)\n",
        "        attn = attn.transpose(1, 2).contiguous().view(B, T, self.cfg.hidden_size)\n",
        "        return self.o_proj(attn)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, cfg: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Linear(cfg.hidden_size, cfg.intermediate_size, bias=False)\n",
        "        self.up = nn.Linear(cfg.hidden_size, cfg.intermediate_size, bias=False)\n",
        "        self.down = nn.Linear(cfg.intermediate_size, cfg.hidden_size, bias=False)\n",
        "    def forward(self, x):\n",
        "        return self.down(F.silu(self.gate(x)) * self.up(x))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, cfg: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)\n",
        "        self.attn = GroupedQueryAttention(cfg)\n",
        "        self.ln2 = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)\n",
        "        self.mlp = MLP(cfg)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SmolLM2Model(nn.Module):\n",
        "    def __init__(self, cfg: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.embed_tokens = nn.Embedding(cfg.vocab_size, cfg.hidden_size)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(cfg) for _ in range(cfg.num_hidden_layers)])\n",
        "        self.ln_f = RMSNorm(cfg.hidden_size, eps=cfg.rms_norm_eps)\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed_tokens(input_ids)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        return x\n",
        "\n",
        "class SmolLM2ForCausalLM(nn.Module):\n",
        "    def __init__(self, config: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model = SmolLM2Model(config)\n",
        "        if config.tie_word_embeddings:\n",
        "            self.lm_head = None\n",
        "        else:\n",
        "            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.apply(self._init_weights)\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        hidden_states = self.model(input_ids)\n",
        "        if self.config.tie_word_embeddings:\n",
        "            logits = F.linear(hidden_states, self.model.embed_tokens.weight)\n",
        "        else:\n",
        "            logits = self.lm_head(hidden_states)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = F.cross_entropy(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_labels.view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "        return logits, loss\n",
        "\n",
        "# =============================\n",
        "# Resume Training\n",
        "# =============================\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    input_file: str = \"input.txt\"\n",
        "    block_size: int = 32\n",
        "    batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 16\n",
        "    learning_rate: float = 6e-4\n",
        "    min_lr: float = 6e-5\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.1\n",
        "    beta1: float = 0.9\n",
        "    beta2: float = 0.95\n",
        "    grad_clip: float = 1.0\n",
        "    checkpoint_dir: str = \"checkpoints\"\n",
        "    save_every: int = 500\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    use_amp: bool = True\n",
        "    resume_checkpoint: str = \"checkpoint_step_5000.pt\"\n",
        "    max_steps: int = 50  # Continue for 50 steps\n",
        "\n",
        "# =============================\n",
        "# Training function\n",
        "# =============================\n",
        "def train_resume():\n",
        "    device, use_amp, amp_dtype, device_name = get_device_config()\n",
        "    cfg = SmolLM2Config.from_json(\"config.json\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "    dataset = TextDataset(TrainingConfig().input_file, tokenizer, block_size=32, batch_size=4)\n",
        "    dataloader = DataLoader(dataset, batch_size=1)\n",
        "    all_batches = [b for b in dataloader]\n",
        "\n",
        "    model = SmolLM2ForCausalLM(cfg).to(device)\n",
        "    try:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig().learning_rate,\n",
        "                                      betas=(TrainingConfig().beta1, TrainingConfig().beta2),\n",
        "                                      weight_decay=TrainingConfig().weight_decay, fused=True)\n",
        "    except:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig().learning_rate,\n",
        "                                      betas=(TrainingConfig().beta1, TrainingConfig().beta2),\n",
        "                                      weight_decay=TrainingConfig().weight_decay)\n",
        "\n",
        "    # Allow loading config object from checkpoint\n",
        "    add_safe_globals([SmolLM2Config])\n",
        "    ckpt = torch.load(TrainingConfig().resume_checkpoint, map_location=device, weights_only=False)\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "    start_step = ckpt[\"global_step\"]\n",
        "\n",
        "    print(f\"Resuming from step {start_step}\")\n",
        "\n",
        "    global_step = start_step\n",
        "    micro_batch_count = 0\n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(total=start_step + TrainingConfig().max_steps, desc=\"Training\", dynamic_ncols=True, initial=start_step)\n",
        "\n",
        "    while global_step < start_step + TrainingConfig().max_steps:\n",
        "        x, y = all_batches[micro_batch_count % len(all_batches)]\n",
        "        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.autocast(device.type, dtype=amp_dtype):\n",
        "                logits, loss = model(x, labels=y)\n",
        "                loss = loss / TrainingConfig().gradient_accumulation_steps\n",
        "        else:\n",
        "            logits, loss = model(x, labels=y)\n",
        "            loss = loss / TrainingConfig().gradient_accumulation_steps\n",
        "\n",
        "        loss.backward()\n",
        "        micro_batch_count += 1\n",
        "\n",
        "        if micro_batch_count % TrainingConfig().gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), TrainingConfig().grad_clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({'loss': f'{loss.item() * TrainingConfig().gradient_accumulation_steps:.4f}'})\n",
        "\n",
        "            # Save checkpoint if needed\n",
        "            if global_step % TrainingConfig().save_every == 0:\n",
        "                os.makedirs(TrainingConfig().checkpoint_dir, exist_ok=True)\n",
        "                checkpoint_path = os.path.join(TrainingConfig().checkpoint_dir, f\"checkpoint_step_{global_step}.pt\")\n",
        "                torch.save({\n",
        "                    'global_step': global_step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'config': cfg\n",
        "                }, checkpoint_path)\n",
        "                print(f\"\\n✓ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    pbar.close()\n",
        "    print(\"✅ Resume training complete!\")\n",
        "\n",
        "# =============================\n",
        "# Run training\n",
        "# =============================\n",
        "if __name__ == \"__main__\":\n",
        "    train_resume()\n"
      ],
      "metadata": {
        "id": "JabnbEIZMtJz",
        "outputId": "076dce70-dfb5-4a6d-cbc1-641e0b9e8741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from step 5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 5050/5050 [01:58<00:00,  2.38s/it, loss=1.0300]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Resume training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}